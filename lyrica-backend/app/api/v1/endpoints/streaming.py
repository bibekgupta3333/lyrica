"""
Streaming endpoints for real-time lyrics and song generation.

This module provides Server-Sent Events (SSE) streaming endpoints
for real-time generation feedback.
"""

from typing import AsyncGenerator

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from loguru import logger
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from app.api.dependencies import get_current_active_user, get_db
from app.models.user import User
from app.services.llm import get_llm_service

router = APIRouter()


# ============================================================================
# Request/Response Models
# ============================================================================


class StreamRequest(BaseModel):
    """Request model for streaming generation."""

    prompt: str = Field(..., min_length=10, max_length=2000)
    genre: str = Field(default="pop")
    mood: str = Field(default="happy")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int = Field(default=500, ge=50, le=2000)
    llm_provider: str = Field(default="ollama")


# ============================================================================
# Streaming Endpoints
# ============================================================================


@router.post("/stream/lyrics")
async def stream_lyrics_generation(
    request: StreamRequest,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db),
):
    """
    Stream lyrics generation in real-time using Server-Sent Events (SSE).

    This endpoint streams the lyrics generation process, sending chunks
    as they are generated by the LLM.

    Args:
        request: Stream request parameters
        current_user: Authenticated user
        db: Database session

    Returns:
        Streaming response with Server-Sent Events

    Example:
        ```bash
        curl -N http://localhost:8000/api/v1/stream/lyrics \
          -H "Authorization: Bearer <token>" \
          -H "Content-Type: application/json" \
          -d '{"prompt": "A song about love", "genre": "pop"}'
        ```

    SSE Format:
        ```
        data: {"type": "start", "message": "Starting generation..."}

        data: {"type": "chunk", "content": "[Verse 1]\\n"}

        data: {"type": "chunk", "content": "When I look into your eyes\\n"}

        data: {"type": "complete", "total_tokens": 150}

        ```
    """

    async def generate_stream() -> AsyncGenerator[str, None]:
        """Generate SSE stream."""
        try:
            # Send start event
            yield f'data: {{"type": "start", "message": "Starting generation..."}}\n\n'

            # Get LLM service
            llm_service = get_llm_service(provider=request.llm_provider)

            # Create prompt
            full_prompt = f"""Generate song lyrics based on this prompt:

Prompt: {request.prompt}
Genre: {request.genre}
Mood: {request.mood}

Generate creative and engaging lyrics with proper structure (verses, chorus, etc.).
"""

            # Stream generation
            token_count = 0
            async for chunk in llm_service.generate_stream(
                prompt=full_prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
            ):
                content = chunk.get("content", "")
                if content:
                    token_count += len(content.split())
                    # Send chunk event
                    yield f'data: {{"type": "chunk", "content": {repr(content)}}}\n\n'

            # Send completion event
            yield f'data: {{"type": "complete", "total_tokens": {token_count}}}\n\n'

            logger.info(f"Stream completed for user {current_user.id}: {token_count} tokens")

        except Exception as e:
            logger.error(f"Streaming error: {str(e)}")
            yield f'data: {{"type": "error", "message": "Generation failed: {str(e)}"}}\n\n'

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


@router.post("/stream/chat")
async def stream_chat(
    request: StreamRequest,
    current_user: User = Depends(get_current_active_user),
):
    """
    Stream chat-based lyrics generation.

    This endpoint provides a conversational interface for lyrics generation
    with streaming responses.

    Args:
        request: Stream request parameters
        current_user: Authenticated user

    Returns:
        Streaming response with Server-Sent Events
    """

    async def generate_chat_stream() -> AsyncGenerator[str, None]:
        """Generate chat SSE stream."""
        try:
            yield f'data: {{"type": "start"}}\n\n'

            # Get LLM service
            llm_service = get_llm_service(provider=request.llm_provider)

            # Create chat messages
            from app.services.llm.base import LLMMessage

            messages = [
                LLMMessage(
                    role="system",
                    content=f"You are a creative song lyrics generator. Generate lyrics in {request.genre} genre with a {request.mood} mood.",
                ),
                LLMMessage(role="user", content=request.prompt),
            ]

            # Stream chat
            async for chunk in llm_service.chat_stream(
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
            ):
                content = chunk.get("content", "")
                if content:
                    yield f'data: {{"type": "chunk", "content": {repr(content)}}}\n\n'

            yield f'data: {{"type": "complete"}}\n\n'

        except Exception as e:
            logger.error(f"Chat streaming error: {str(e)}")
            yield f'data: {{"type": "error", "message": {repr(str(e))}}}\n\n'

    return StreamingResponse(
        generate_chat_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )
